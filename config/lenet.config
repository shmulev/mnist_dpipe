# from dpipe_configs.assets.core import *
from .dataset import *

import dpipe.commands as commands
from dpipe.io import ConsoleArguments, load_json
from dpipe.experiment.flat.base import flat
from dpipe.train.train import train_base
from dpipe.train.validator import validate

from dpipe.train.batch_iter import make_batch_iter_from_finite
from dpipe.train.lr_base import LearningRatePolicy
from dpipe.tf.model import TFModel, TFFrozenModel
from dpipe.tf.utils import get_tf_optimizer, softmax, softmax_cross_entropy

from mnist_dpipe.mnist_utils import validate_fn, val_metrics
from mnist_dpipe.batch_iter.simple import simple_iterator
from mnist_dpipe.model_core.lenet import LeNet
from mnist_dpipe.batch_predict.simple import SimpleBatchPredict

console = ConsoleArguments()

train_ids = load_json(path=console.train_ids_path)
val_ids = load_json(path=console.val_ids_path)
ids = load_json(path=console.ids_path)

save_model_path = console.save_model_path
restore_model_path = console.restore_model_path

build_experiment = flat(
    makefile=makefile,
    config_path=console.config_path,
    experiment_path=console.experiment_path,
    split=split
)

train_model = commands.train_model(
    train=train,
    model=model,
    save_model_path=save_model_path
)

predict = commands.predict(
    ids=ids,
    output_path=console.output_path,
    load_x=load_x,
    frozen_model=frozen_model,
    batch_predict=batch_predict
)

batch_predict = SimpleBatchPredict

evaluate = commands.evaluate(
    load_y=load_y,
    input_path=console.input_path,
    output_path=console.output_path,
    ids=ids,
    metrics=val_metrics
)

log_path = console.log_path

train = train_base(
    # lazy
    model=model,
    batch_iter=batch_iter,
    n_epochs=n_epochs,
    log_path=log_path,
    lr_policy=lr_policy,
    validate=validate(
        # lazy
        load_x=load_x,
        load_y=load_y,
        ids=val_ids,
        metrics=val_metrics,
        validate_fn=validate_fn(
            # lazy
            model=model
        )
    )
)

load_x = dataset.load_image
load_y = dataset.load_label

batch_iter = make_batch_iter_from_finite(
    get_batch_iter=simple_iterator(
        # lazy
        ids=train_ids,
        load_x=load_x,
        load_y=load_y,
        batch_size=batch_size,
        shuffle=True
    )
)

lr_policy = LearningRatePolicy(lr_init)

optimize = get_tf_optimizer(
    # lazy
    tf_optimizer_name='AdamOptimizer',
    beta1=0.899
)

model_core = LeNet(
    image_size=image_size,
    n_chans_img=n_chans_img,
    classes=classes
)

model = TFModel(
    model_core=model_core,
    logits2pred=logits2pred,
    logits2loss=logits2loss,
    optimize=optimize,
)

frozen_model = TFFrozenModel(
    model_core=model_core,
    logits2pred=logits2pred,
    restore_model_path=restore_model_path
)

logits2pred = softmax
logits2loss = softmax_cross_entropy

n_epochs = 10
batch_size = 128
lr_init = 1e-3
image_size = 28
n_chans_img = 1
classes = 10

makefile = "train_predict_evaluate"